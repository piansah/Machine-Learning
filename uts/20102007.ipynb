{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>x11</th>\n",
       "      <th>x12</th>\n",
       "      <th>x13</th>\n",
       "      <th>x14</th>\n",
       "      <th>x15</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.318114</td>\n",
       "      <td>0.817628</td>\n",
       "      <td>0.478107</td>\n",
       "      <td>0.959454</td>\n",
       "      <td>0.642358</td>\n",
       "      <td>0.452873</td>\n",
       "      <td>0.776778</td>\n",
       "      <td>0.028347</td>\n",
       "      <td>0.871464</td>\n",
       "      <td>0.003371</td>\n",
       "      <td>0.808647</td>\n",
       "      <td>0.925931</td>\n",
       "      <td>0.010488</td>\n",
       "      <td>0.723321</td>\n",
       "      <td>0.978188</td>\n",
       "      <td>12.643120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.509037</td>\n",
       "      <td>0.905093</td>\n",
       "      <td>0.029438</td>\n",
       "      <td>0.951064</td>\n",
       "      <td>0.851087</td>\n",
       "      <td>0.995833</td>\n",
       "      <td>0.958588</td>\n",
       "      <td>0.653922</td>\n",
       "      <td>0.061056</td>\n",
       "      <td>0.129264</td>\n",
       "      <td>0.226574</td>\n",
       "      <td>0.454752</td>\n",
       "      <td>0.537741</td>\n",
       "      <td>0.086267</td>\n",
       "      <td>0.349434</td>\n",
       "      <td>3.530959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.810271</td>\n",
       "      <td>0.876971</td>\n",
       "      <td>0.224786</td>\n",
       "      <td>0.802447</td>\n",
       "      <td>0.748440</td>\n",
       "      <td>0.390927</td>\n",
       "      <td>0.998609</td>\n",
       "      <td>0.959626</td>\n",
       "      <td>0.256438</td>\n",
       "      <td>0.276695</td>\n",
       "      <td>0.303964</td>\n",
       "      <td>0.037177</td>\n",
       "      <td>0.201011</td>\n",
       "      <td>0.902961</td>\n",
       "      <td>0.928177</td>\n",
       "      <td>9.894655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.376955</td>\n",
       "      <td>0.445994</td>\n",
       "      <td>0.118143</td>\n",
       "      <td>0.586999</td>\n",
       "      <td>0.649965</td>\n",
       "      <td>0.096579</td>\n",
       "      <td>0.988305</td>\n",
       "      <td>0.049376</td>\n",
       "      <td>0.064940</td>\n",
       "      <td>0.791164</td>\n",
       "      <td>0.860031</td>\n",
       "      <td>0.480225</td>\n",
       "      <td>0.858464</td>\n",
       "      <td>0.748295</td>\n",
       "      <td>0.417591</td>\n",
       "      <td>1.378890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.853618</td>\n",
       "      <td>0.815781</td>\n",
       "      <td>0.747351</td>\n",
       "      <td>0.396136</td>\n",
       "      <td>0.745253</td>\n",
       "      <td>0.643946</td>\n",
       "      <td>0.895392</td>\n",
       "      <td>0.572550</td>\n",
       "      <td>0.922699</td>\n",
       "      <td>0.400879</td>\n",
       "      <td>0.896344</td>\n",
       "      <td>0.858500</td>\n",
       "      <td>0.414127</td>\n",
       "      <td>0.868506</td>\n",
       "      <td>0.933232</td>\n",
       "      <td>9.615164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>0.576147</td>\n",
       "      <td>0.746749</td>\n",
       "      <td>0.230601</td>\n",
       "      <td>0.948815</td>\n",
       "      <td>0.518976</td>\n",
       "      <td>0.570783</td>\n",
       "      <td>0.163107</td>\n",
       "      <td>0.704152</td>\n",
       "      <td>0.753967</td>\n",
       "      <td>0.864234</td>\n",
       "      <td>0.024401</td>\n",
       "      <td>0.350367</td>\n",
       "      <td>0.253657</td>\n",
       "      <td>0.079964</td>\n",
       "      <td>0.841069</td>\n",
       "      <td>3.818416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>0.247844</td>\n",
       "      <td>0.158727</td>\n",
       "      <td>0.627563</td>\n",
       "      <td>0.254203</td>\n",
       "      <td>0.810601</td>\n",
       "      <td>0.688892</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.053736</td>\n",
       "      <td>0.687611</td>\n",
       "      <td>0.129055</td>\n",
       "      <td>0.914624</td>\n",
       "      <td>0.679960</td>\n",
       "      <td>0.418746</td>\n",
       "      <td>0.185899</td>\n",
       "      <td>0.863095</td>\n",
       "      <td>8.344037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>0.282080</td>\n",
       "      <td>0.880512</td>\n",
       "      <td>0.853496</td>\n",
       "      <td>0.156204</td>\n",
       "      <td>0.190641</td>\n",
       "      <td>0.429884</td>\n",
       "      <td>0.559712</td>\n",
       "      <td>0.965087</td>\n",
       "      <td>0.344490</td>\n",
       "      <td>0.480646</td>\n",
       "      <td>0.056572</td>\n",
       "      <td>0.897596</td>\n",
       "      <td>0.015407</td>\n",
       "      <td>0.430169</td>\n",
       "      <td>0.078517</td>\n",
       "      <td>2.488362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>0.721762</td>\n",
       "      <td>0.879809</td>\n",
       "      <td>0.610347</td>\n",
       "      <td>0.955372</td>\n",
       "      <td>0.170584</td>\n",
       "      <td>0.379517</td>\n",
       "      <td>0.732656</td>\n",
       "      <td>0.387099</td>\n",
       "      <td>0.777541</td>\n",
       "      <td>0.393373</td>\n",
       "      <td>0.402806</td>\n",
       "      <td>0.858849</td>\n",
       "      <td>0.038505</td>\n",
       "      <td>0.204338</td>\n",
       "      <td>0.043047</td>\n",
       "      <td>3.577130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>0.769111</td>\n",
       "      <td>0.370680</td>\n",
       "      <td>0.670094</td>\n",
       "      <td>0.779518</td>\n",
       "      <td>0.902451</td>\n",
       "      <td>0.999762</td>\n",
       "      <td>0.285822</td>\n",
       "      <td>0.054684</td>\n",
       "      <td>0.429217</td>\n",
       "      <td>0.746632</td>\n",
       "      <td>0.112306</td>\n",
       "      <td>0.452160</td>\n",
       "      <td>0.024094</td>\n",
       "      <td>0.252050</td>\n",
       "      <td>0.023717</td>\n",
       "      <td>-2.464826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x1        x2        x3        x4        x5        x6        x7  \\\n",
       "0    0.318114  0.817628  0.478107  0.959454  0.642358  0.452873  0.776778   \n",
       "1    0.509037  0.905093  0.029438  0.951064  0.851087  0.995833  0.958588   \n",
       "2    0.810271  0.876971  0.224786  0.802447  0.748440  0.390927  0.998609   \n",
       "3    0.376955  0.445994  0.118143  0.586999  0.649965  0.096579  0.988305   \n",
       "4    0.853618  0.815781  0.747351  0.396136  0.745253  0.643946  0.895392   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "895  0.576147  0.746749  0.230601  0.948815  0.518976  0.570783  0.163107   \n",
       "896  0.247844  0.158727  0.627563  0.254203  0.810601  0.688892  0.061867   \n",
       "897  0.282080  0.880512  0.853496  0.156204  0.190641  0.429884  0.559712   \n",
       "898  0.721762  0.879809  0.610347  0.955372  0.170584  0.379517  0.732656   \n",
       "899  0.769111  0.370680  0.670094  0.779518  0.902451  0.999762  0.285822   \n",
       "\n",
       "           x8        x9       x10       x11       x12       x13       x14  \\\n",
       "0    0.028347  0.871464  0.003371  0.808647  0.925931  0.010488  0.723321   \n",
       "1    0.653922  0.061056  0.129264  0.226574  0.454752  0.537741  0.086267   \n",
       "2    0.959626  0.256438  0.276695  0.303964  0.037177  0.201011  0.902961   \n",
       "3    0.049376  0.064940  0.791164  0.860031  0.480225  0.858464  0.748295   \n",
       "4    0.572550  0.922699  0.400879  0.896344  0.858500  0.414127  0.868506   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "895  0.704152  0.753967  0.864234  0.024401  0.350367  0.253657  0.079964   \n",
       "896  0.053736  0.687611  0.129055  0.914624  0.679960  0.418746  0.185899   \n",
       "897  0.965087  0.344490  0.480646  0.056572  0.897596  0.015407  0.430169   \n",
       "898  0.387099  0.777541  0.393373  0.402806  0.858849  0.038505  0.204338   \n",
       "899  0.054684  0.429217  0.746632  0.112306  0.452160  0.024094  0.252050   \n",
       "\n",
       "          x15          y  \n",
       "0    0.978188  12.643120  \n",
       "1    0.349434   3.530959  \n",
       "2    0.928177   9.894655  \n",
       "3    0.417591   1.378890  \n",
       "4    0.933232   9.615164  \n",
       "..        ...        ...  \n",
       "895  0.841069   3.818416  \n",
       "896  0.863095   8.344037  \n",
       "897  0.078517   2.488362  \n",
       "898  0.043047   3.577130  \n",
       "899  0.023717  -2.464826  \n",
       "\n",
       "[900 rows x 16 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('uts_train.csv')\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_train.values[:, :-1]\n",
    "y = df_train.values[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 60)                960       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 45)                2745      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 30)                1380      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 15)                465       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 16        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,566\n",
      "Trainable params: 5,566\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(60, input_dim=15, activation='relu'))\n",
    "model.add(Dense(45, activation='relu'))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(15, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam',metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "255/255 [==============================] - 3s 5ms/step - loss: 7.0641 - mae: 2.0141 - val_loss: 1.4042 - val_mae: 0.9305\n",
      "Epoch 2/50\n",
      "255/255 [==============================] - 1s 3ms/step - loss: 1.4607 - mae: 0.9055 - val_loss: 1.2480 - val_mae: 0.9064\n",
      "Epoch 3/50\n",
      "255/255 [==============================] - 1s 3ms/step - loss: 1.2006 - mae: 0.8581 - val_loss: 0.9529 - val_mae: 0.7831\n",
      "Epoch 4/50\n",
      "255/255 [==============================] - 1s 3ms/step - loss: 0.8834 - mae: 0.7353 - val_loss: 0.9362 - val_mae: 0.7649\n",
      "Epoch 5/50\n",
      "255/255 [==============================] - 1s 3ms/step - loss: 0.7352 - mae: 0.6818 - val_loss: 0.7817 - val_mae: 0.6960\n",
      "Epoch 6/50\n",
      "255/255 [==============================] - 1s 3ms/step - loss: 0.5711 - mae: 0.5985 - val_loss: 0.7472 - val_mae: 0.6951\n",
      "Epoch 7/50\n",
      "255/255 [==============================] - 1s 3ms/step - loss: 0.4838 - mae: 0.5422 - val_loss: 0.5907 - val_mae: 0.6075\n",
      "Epoch 8/50\n",
      "255/255 [==============================] - 1s 3ms/step - loss: 0.4087 - mae: 0.5076 - val_loss: 0.6064 - val_mae: 0.6316\n",
      "Epoch 9/50\n",
      "255/255 [==============================] - 1s 4ms/step - loss: 0.4332 - mae: 0.5288 - val_loss: 0.5044 - val_mae: 0.5621\n",
      "Epoch 10/50\n",
      "255/255 [==============================] - 1s 4ms/step - loss: 0.3165 - mae: 0.4500 - val_loss: 0.4468 - val_mae: 0.5405\n",
      "Epoch 11/50\n",
      "255/255 [==============================] - 1s 3ms/step - loss: 0.2491 - mae: 0.3872 - val_loss: 0.3220 - val_mae: 0.4653\n",
      "Epoch 12/50\n",
      "255/255 [==============================] - 1s 4ms/step - loss: 0.2840 - mae: 0.4168 - val_loss: 0.5915 - val_mae: 0.6391\n",
      "Epoch 13/50\n",
      "255/255 [==============================] - 1s 4ms/step - loss: 0.2294 - mae: 0.3758 - val_loss: 0.2562 - val_mae: 0.4029\n",
      "Epoch 14/50\n",
      "255/255 [==============================] - 1s 4ms/step - loss: 0.1892 - mae: 0.3416 - val_loss: 0.3440 - val_mae: 0.4518\n",
      "Epoch 15/50\n",
      "255/255 [==============================] - 1s 4ms/step - loss: 0.1828 - mae: 0.3454 - val_loss: 0.5354 - val_mae: 0.6093\n",
      "Epoch 16/50\n",
      "255/255 [==============================] - 1s 3ms/step - loss: 0.1570 - mae: 0.3140 - val_loss: 0.2935 - val_mae: 0.4336\n",
      "Epoch 17/50\n",
      "255/255 [==============================] - 1s 3ms/step - loss: 0.1560 - mae: 0.3079 - val_loss: 0.2600 - val_mae: 0.4010\n",
      "Epoch 18/50\n",
      "255/255 [==============================] - 1s 3ms/step - loss: 0.1624 - mae: 0.3176 - val_loss: 0.2243 - val_mae: 0.3723\n",
      "Epoch 19/50\n",
      "255/255 [==============================] - 1s 3ms/step - loss: 0.1446 - mae: 0.3023 - val_loss: 0.2001 - val_mae: 0.3452\n",
      "Epoch 20/50\n",
      "255/255 [==============================] - 1s 4ms/step - loss: 0.1605 - mae: 0.3164 - val_loss: 0.2535 - val_mae: 0.4154\n",
      "Epoch 21/50\n",
      "255/255 [==============================] - 1s 4ms/step - loss: 0.1442 - mae: 0.3004 - val_loss: 0.5242 - val_mae: 0.6007\n",
      "Epoch 22/50\n",
      "255/255 [==============================] - 1s 4ms/step - loss: 0.1477 - mae: 0.3067 - val_loss: 0.1991 - val_mae: 0.3516\n",
      "Epoch 23/50\n",
      "255/255 [==============================] - 1s 4ms/step - loss: 0.1296 - mae: 0.2847 - val_loss: 0.3166 - val_mae: 0.4612\n",
      "Epoch 24/50\n",
      "255/255 [==============================] - 1s 4ms/step - loss: 0.1334 - mae: 0.2832 - val_loss: 0.2495 - val_mae: 0.4086\n",
      "Epoch 25/50\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1278 - mae: 0.2795 - val_loss: 0.1937 - val_mae: 0.3549\n",
      "Epoch 26/50\n",
      "255/255 [==============================] - 1s 4ms/step - loss: 0.1136 - mae: 0.2622 - val_loss: 0.1723 - val_mae: 0.3263\n",
      "Epoch 27/50\n",
      "255/255 [==============================] - 1s 6ms/step - loss: 0.0969 - mae: 0.2443 - val_loss: 0.1571 - val_mae: 0.3260\n",
      "Epoch 28/50\n",
      "255/255 [==============================] - 1s 6ms/step - loss: 0.1040 - mae: 0.2525 - val_loss: 0.1862 - val_mae: 0.3442\n",
      "Epoch 29/50\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0915 - mae: 0.2364 - val_loss: 0.1555 - val_mae: 0.3127\n",
      "Epoch 30/50\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1048 - mae: 0.2555 - val_loss: 0.2293 - val_mae: 0.3843\n",
      "Epoch 31/50\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0928 - mae: 0.2458 - val_loss: 0.1870 - val_mae: 0.3435\n",
      "Epoch 32/50\n",
      "255/255 [==============================] - 1s 4ms/step - loss: 0.1072 - mae: 0.2585 - val_loss: 0.1682 - val_mae: 0.3283\n",
      "Epoch 33/50\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0834 - mae: 0.2307 - val_loss: 0.1468 - val_mae: 0.3063\n",
      "Epoch 34/50\n",
      "255/255 [==============================] - 1s 3ms/step - loss: 0.0780 - mae: 0.2202 - val_loss: 0.1360 - val_mae: 0.3038\n",
      "Epoch 35/50\n",
      "255/255 [==============================] - 1s 3ms/step - loss: 0.0913 - mae: 0.2356 - val_loss: 0.1317 - val_mae: 0.3014\n",
      "Epoch 36/50\n",
      "255/255 [==============================] - 1s 2ms/step - loss: 0.0920 - mae: 0.2382 - val_loss: 0.1703 - val_mae: 0.3225\n",
      "Epoch 37/50\n",
      "255/255 [==============================] - 1s 2ms/step - loss: 0.0770 - mae: 0.2192 - val_loss: 0.2237 - val_mae: 0.3833\n",
      "Epoch 38/50\n",
      "255/255 [==============================] - 1s 3ms/step - loss: 0.1037 - mae: 0.2556 - val_loss: 0.2028 - val_mae: 0.3594\n",
      "Epoch 39/50\n",
      "255/255 [==============================] - 1s 3ms/step - loss: 0.0668 - mae: 0.2046 - val_loss: 0.1935 - val_mae: 0.3496\n",
      "Epoch 40/50\n",
      "255/255 [==============================] - 1s 2ms/step - loss: 0.0742 - mae: 0.2161 - val_loss: 0.1689 - val_mae: 0.3198\n",
      "Epoch 41/50\n",
      "255/255 [==============================] - 1s 3ms/step - loss: 0.0769 - mae: 0.2201 - val_loss: 0.2547 - val_mae: 0.4174\n",
      "Epoch 42/50\n",
      "255/255 [==============================] - 1s 3ms/step - loss: 0.1000 - mae: 0.2426 - val_loss: 0.1310 - val_mae: 0.3037\n",
      "Epoch 43/50\n",
      "255/255 [==============================] - 1s 3ms/step - loss: 0.0611 - mae: 0.1957 - val_loss: 0.1815 - val_mae: 0.3405\n",
      "Epoch 44/50\n",
      "255/255 [==============================] - 1s 3ms/step - loss: 0.0597 - mae: 0.1951 - val_loss: 0.1884 - val_mae: 0.3430\n",
      "Epoch 45/50\n",
      "255/255 [==============================] - 1s 3ms/step - loss: 0.0718 - mae: 0.2103 - val_loss: 0.1248 - val_mae: 0.2791\n",
      "Epoch 46/50\n",
      "255/255 [==============================] - 1s 3ms/step - loss: 0.0496 - mae: 0.1783 - val_loss: 0.1241 - val_mae: 0.2850\n",
      "Epoch 47/50\n",
      "255/255 [==============================] - 1s 3ms/step - loss: 0.0652 - mae: 0.1974 - val_loss: 0.1278 - val_mae: 0.2836\n",
      "Epoch 48/50\n",
      "255/255 [==============================] - 1s 2ms/step - loss: 0.0670 - mae: 0.2058 - val_loss: 0.1988 - val_mae: 0.3480\n",
      "Epoch 49/50\n",
      "255/255 [==============================] - 1s 2ms/step - loss: 0.0931 - mae: 0.2318 - val_loss: 0.1326 - val_mae: 0.2933\n",
      "Epoch 50/50\n",
      "255/255 [==============================] - 1s 3ms/step - loss: 0.0529 - mae: 0.1836 - val_loss: 0.1256 - val_mae: 0.2832\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size = 3,\n",
    "    epochs = 50,\n",
    "    validation_split = 0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.009714126586914,\n",
       " 2.309828519821167,\n",
       " 10.615478515625,\n",
       " 8.093951225280762,\n",
       " 1.489666223526001,\n",
       " 3.6751582622528076,\n",
       " 0.8883546590805054,\n",
       " 4.200063228607178,\n",
       " 8.622628211975098,\n",
       " 2.220015048980713,\n",
       " 2.7749440670013428,\n",
       " 3.024430274963379,\n",
       " 6.710522651672363,\n",
       " -4.001375198364258,\n",
       " 8.503776550292969,\n",
       " 6.39858341217041,\n",
       " 7.402189254760742,\n",
       " 9.688898086547852,\n",
       " 9.707883834838867,\n",
       " -0.861849308013916,\n",
       " 7.157867908477783,\n",
       " 1.5935897827148438,\n",
       " -1.079371452331543,\n",
       " -3.489315986633301,\n",
       " 2.1544620990753174,\n",
       " -2.082446813583374,\n",
       " 2.7454395294189453,\n",
       " 1.1715037822723389,\n",
       " 0.5589247941970825,\n",
       " 4.2641987800598145,\n",
       " 10.23737621307373,\n",
       " 9.172961235046387,\n",
       " 2.77398419380188,\n",
       " 10.498278617858887,\n",
       " 7.773684501647949,\n",
       " 4.348668098449707,\n",
       " 5.248842716217041,\n",
       " 0.009583011269569397,\n",
       " 6.163266658782959,\n",
       " -3.760021924972534,\n",
       " 4.192142963409424,\n",
       " 7.279123783111572,\n",
       " 7.826292991638184,\n",
       " 7.271766662597656,\n",
       " -0.20628467202186584,\n",
       " 3.2068727016448975,\n",
       " 0.42569655179977417,\n",
       " 5.829389572143555,\n",
       " -0.01626688241958618,\n",
       " 0.3345433473587036,\n",
       " 0.36657750606536865,\n",
       " 2.102402448654175,\n",
       " 5.415136337280273,\n",
       " 5.1660027503967285,\n",
       " 4.804390907287598,\n",
       " -3.4515905380249023,\n",
       " 6.581381797790527,\n",
       " 6.809730052947998,\n",
       " 5.749003887176514,\n",
       " 0.7563034296035767,\n",
       " 8.815032005310059,\n",
       " 6.170392036437988,\n",
       " 5.279331207275391,\n",
       " 9.440773963928223,\n",
       " 2.5639452934265137,\n",
       " 7.357892990112305,\n",
       " 7.535175800323486,\n",
       " 4.549252510070801,\n",
       " 1.7096102237701416,\n",
       " 5.378262519836426,\n",
       " 6.901254653930664,\n",
       " -0.5783605575561523,\n",
       " 4.250007152557373,\n",
       " 0.48920738697052,\n",
       " -0.8774147033691406,\n",
       " 8.376187324523926,\n",
       " 3.664849281311035,\n",
       " 1.3497189283370972,\n",
       " -0.1482050120830536,\n",
       " 4.76696252822876,\n",
       " 10.489383697509766,\n",
       " 2.048586130142212,\n",
       " 3.9145455360412598,\n",
       " 7.59329080581665,\n",
       " 4.1041364669799805,\n",
       " 2.6376047134399414,\n",
       " 8.993863105773926,\n",
       " -0.7171739339828491,\n",
       " 2.675492525100708,\n",
       " 10.213565826416016,\n",
       " 4.94742488861084,\n",
       " 2.316816568374634,\n",
       " 5.549903392791748,\n",
       " 6.81944465637207,\n",
       " 9.08400821685791,\n",
       " 10.276219367980957,\n",
       " 0.7200665473937988,\n",
       " 7.991176605224609,\n",
       " -0.0264480859041214,\n",
       " 6.695094585418701]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = y_pred.tolist()\n",
    "y_pred = [y_pred[i][0] for i in range(len(y_pred))]\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasil_akhir = pd.DataFrame({\"y\" : y_pred})\n",
    "hasil_akhir.to_csv(\"20102007.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('python')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45bf7f17dadebdc581538a04e72eec4af353140a24cf5b2bc98c1d568cfdbf7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
