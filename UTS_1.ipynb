{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>x11</th>\n",
       "      <th>x12</th>\n",
       "      <th>x13</th>\n",
       "      <th>x14</th>\n",
       "      <th>x15</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.318114</td>\n",
       "      <td>0.817628</td>\n",
       "      <td>0.478107</td>\n",
       "      <td>0.959454</td>\n",
       "      <td>0.642358</td>\n",
       "      <td>0.452873</td>\n",
       "      <td>0.776778</td>\n",
       "      <td>0.028347</td>\n",
       "      <td>0.871464</td>\n",
       "      <td>0.003371</td>\n",
       "      <td>0.808647</td>\n",
       "      <td>0.925931</td>\n",
       "      <td>0.010488</td>\n",
       "      <td>0.723321</td>\n",
       "      <td>0.978188</td>\n",
       "      <td>12.643120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.509037</td>\n",
       "      <td>0.905093</td>\n",
       "      <td>0.029438</td>\n",
       "      <td>0.951064</td>\n",
       "      <td>0.851087</td>\n",
       "      <td>0.995833</td>\n",
       "      <td>0.958588</td>\n",
       "      <td>0.653922</td>\n",
       "      <td>0.061056</td>\n",
       "      <td>0.129264</td>\n",
       "      <td>0.226574</td>\n",
       "      <td>0.454752</td>\n",
       "      <td>0.537741</td>\n",
       "      <td>0.086267</td>\n",
       "      <td>0.349434</td>\n",
       "      <td>3.530959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.810271</td>\n",
       "      <td>0.876971</td>\n",
       "      <td>0.224786</td>\n",
       "      <td>0.802447</td>\n",
       "      <td>0.748440</td>\n",
       "      <td>0.390927</td>\n",
       "      <td>0.998609</td>\n",
       "      <td>0.959626</td>\n",
       "      <td>0.256438</td>\n",
       "      <td>0.276695</td>\n",
       "      <td>0.303964</td>\n",
       "      <td>0.037177</td>\n",
       "      <td>0.201011</td>\n",
       "      <td>0.902961</td>\n",
       "      <td>0.928177</td>\n",
       "      <td>9.894655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.376955</td>\n",
       "      <td>0.445994</td>\n",
       "      <td>0.118143</td>\n",
       "      <td>0.586999</td>\n",
       "      <td>0.649965</td>\n",
       "      <td>0.096579</td>\n",
       "      <td>0.988305</td>\n",
       "      <td>0.049376</td>\n",
       "      <td>0.064940</td>\n",
       "      <td>0.791164</td>\n",
       "      <td>0.860031</td>\n",
       "      <td>0.480225</td>\n",
       "      <td>0.858464</td>\n",
       "      <td>0.748295</td>\n",
       "      <td>0.417591</td>\n",
       "      <td>1.378890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.853618</td>\n",
       "      <td>0.815781</td>\n",
       "      <td>0.747351</td>\n",
       "      <td>0.396136</td>\n",
       "      <td>0.745253</td>\n",
       "      <td>0.643946</td>\n",
       "      <td>0.895392</td>\n",
       "      <td>0.572550</td>\n",
       "      <td>0.922699</td>\n",
       "      <td>0.400879</td>\n",
       "      <td>0.896344</td>\n",
       "      <td>0.858500</td>\n",
       "      <td>0.414127</td>\n",
       "      <td>0.868506</td>\n",
       "      <td>0.933232</td>\n",
       "      <td>9.615164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>0.576147</td>\n",
       "      <td>0.746749</td>\n",
       "      <td>0.230601</td>\n",
       "      <td>0.948815</td>\n",
       "      <td>0.518976</td>\n",
       "      <td>0.570783</td>\n",
       "      <td>0.163107</td>\n",
       "      <td>0.704152</td>\n",
       "      <td>0.753967</td>\n",
       "      <td>0.864234</td>\n",
       "      <td>0.024401</td>\n",
       "      <td>0.350367</td>\n",
       "      <td>0.253657</td>\n",
       "      <td>0.079964</td>\n",
       "      <td>0.841069</td>\n",
       "      <td>3.818416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>0.247844</td>\n",
       "      <td>0.158727</td>\n",
       "      <td>0.627563</td>\n",
       "      <td>0.254203</td>\n",
       "      <td>0.810601</td>\n",
       "      <td>0.688892</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.053736</td>\n",
       "      <td>0.687611</td>\n",
       "      <td>0.129055</td>\n",
       "      <td>0.914624</td>\n",
       "      <td>0.679960</td>\n",
       "      <td>0.418746</td>\n",
       "      <td>0.185899</td>\n",
       "      <td>0.863095</td>\n",
       "      <td>8.344037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>0.282080</td>\n",
       "      <td>0.880512</td>\n",
       "      <td>0.853496</td>\n",
       "      <td>0.156204</td>\n",
       "      <td>0.190641</td>\n",
       "      <td>0.429884</td>\n",
       "      <td>0.559712</td>\n",
       "      <td>0.965087</td>\n",
       "      <td>0.344490</td>\n",
       "      <td>0.480646</td>\n",
       "      <td>0.056572</td>\n",
       "      <td>0.897596</td>\n",
       "      <td>0.015407</td>\n",
       "      <td>0.430169</td>\n",
       "      <td>0.078517</td>\n",
       "      <td>2.488362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>0.721762</td>\n",
       "      <td>0.879809</td>\n",
       "      <td>0.610347</td>\n",
       "      <td>0.955372</td>\n",
       "      <td>0.170584</td>\n",
       "      <td>0.379517</td>\n",
       "      <td>0.732656</td>\n",
       "      <td>0.387099</td>\n",
       "      <td>0.777541</td>\n",
       "      <td>0.393373</td>\n",
       "      <td>0.402806</td>\n",
       "      <td>0.858849</td>\n",
       "      <td>0.038505</td>\n",
       "      <td>0.204338</td>\n",
       "      <td>0.043047</td>\n",
       "      <td>3.577130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>0.769111</td>\n",
       "      <td>0.370680</td>\n",
       "      <td>0.670094</td>\n",
       "      <td>0.779518</td>\n",
       "      <td>0.902451</td>\n",
       "      <td>0.999762</td>\n",
       "      <td>0.285822</td>\n",
       "      <td>0.054684</td>\n",
       "      <td>0.429217</td>\n",
       "      <td>0.746632</td>\n",
       "      <td>0.112306</td>\n",
       "      <td>0.452160</td>\n",
       "      <td>0.024094</td>\n",
       "      <td>0.252050</td>\n",
       "      <td>0.023717</td>\n",
       "      <td>-2.464826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x1        x2        x3        x4        x5        x6        x7  \\\n",
       "0    0.318114  0.817628  0.478107  0.959454  0.642358  0.452873  0.776778   \n",
       "1    0.509037  0.905093  0.029438  0.951064  0.851087  0.995833  0.958588   \n",
       "2    0.810271  0.876971  0.224786  0.802447  0.748440  0.390927  0.998609   \n",
       "3    0.376955  0.445994  0.118143  0.586999  0.649965  0.096579  0.988305   \n",
       "4    0.853618  0.815781  0.747351  0.396136  0.745253  0.643946  0.895392   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "895  0.576147  0.746749  0.230601  0.948815  0.518976  0.570783  0.163107   \n",
       "896  0.247844  0.158727  0.627563  0.254203  0.810601  0.688892  0.061867   \n",
       "897  0.282080  0.880512  0.853496  0.156204  0.190641  0.429884  0.559712   \n",
       "898  0.721762  0.879809  0.610347  0.955372  0.170584  0.379517  0.732656   \n",
       "899  0.769111  0.370680  0.670094  0.779518  0.902451  0.999762  0.285822   \n",
       "\n",
       "           x8        x9       x10       x11       x12       x13       x14  \\\n",
       "0    0.028347  0.871464  0.003371  0.808647  0.925931  0.010488  0.723321   \n",
       "1    0.653922  0.061056  0.129264  0.226574  0.454752  0.537741  0.086267   \n",
       "2    0.959626  0.256438  0.276695  0.303964  0.037177  0.201011  0.902961   \n",
       "3    0.049376  0.064940  0.791164  0.860031  0.480225  0.858464  0.748295   \n",
       "4    0.572550  0.922699  0.400879  0.896344  0.858500  0.414127  0.868506   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "895  0.704152  0.753967  0.864234  0.024401  0.350367  0.253657  0.079964   \n",
       "896  0.053736  0.687611  0.129055  0.914624  0.679960  0.418746  0.185899   \n",
       "897  0.965087  0.344490  0.480646  0.056572  0.897596  0.015407  0.430169   \n",
       "898  0.387099  0.777541  0.393373  0.402806  0.858849  0.038505  0.204338   \n",
       "899  0.054684  0.429217  0.746632  0.112306  0.452160  0.024094  0.252050   \n",
       "\n",
       "          x15          y  \n",
       "0    0.978188  12.643120  \n",
       "1    0.349434   3.530959  \n",
       "2    0.928177   9.894655  \n",
       "3    0.417591   1.378890  \n",
       "4    0.933232   9.615164  \n",
       "..        ...        ...  \n",
       "895  0.841069   3.818416  \n",
       "896  0.863095   8.344037  \n",
       "897  0.078517   2.488362  \n",
       "898  0.043047   3.577130  \n",
       "899  0.023717  -2.464826  \n",
       "\n",
       "[900 rows x 16 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('uts_train.csv')\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 16)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_train.values[:, :-1]\n",
    "y = df_train.values[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 80)                1280      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 60)                4860      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 40)                2440      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 20)                820       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,421\n",
      "Trainable params: 9,421\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(80, input_dim=15, activation='relu'))\n",
    "model.add(Dense(60, activation='relu'))\n",
    "model.add(Dense(40, activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam',metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.0437 - mae: 0.1641 - val_loss: 0.1588 - val_mae: 0.3025\n",
      "Epoch 2/50\n",
      "765/765 [==============================] - 3s 3ms/step - loss: 0.0312 - mae: 0.1364 - val_loss: 0.0986 - val_mae: 0.2479\n",
      "Epoch 3/50\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.0371 - mae: 0.1486 - val_loss: 0.0964 - val_mae: 0.2376\n",
      "Epoch 4/50\n",
      "765/765 [==============================] - 4s 6ms/step - loss: 0.0565 - mae: 0.1774 - val_loss: 0.1169 - val_mae: 0.2707\n",
      "Epoch 5/50\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.0386 - mae: 0.1559 - val_loss: 0.0993 - val_mae: 0.2432\n",
      "Epoch 6/50\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.0283 - mae: 0.1336 - val_loss: 0.0988 - val_mae: 0.2492\n",
      "Epoch 7/50\n",
      "765/765 [==============================] - 2s 3ms/step - loss: 0.0278 - mae: 0.1278 - val_loss: 0.1091 - val_mae: 0.2655\n",
      "Epoch 8/50\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.0495 - mae: 0.1707 - val_loss: 0.1032 - val_mae: 0.2550\n",
      "Epoch 9/50\n",
      "765/765 [==============================] - 2s 3ms/step - loss: 0.0493 - mae: 0.1678 - val_loss: 0.1050 - val_mae: 0.2541\n",
      "Epoch 10/50\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.0328 - mae: 0.1410 - val_loss: 0.0703 - val_mae: 0.2156\n",
      "Epoch 11/50\n",
      "765/765 [==============================] - 3s 3ms/step - loss: 0.0419 - mae: 0.1571 - val_loss: 0.0716 - val_mae: 0.2133\n",
      "Epoch 12/50\n",
      "765/765 [==============================] - 2s 3ms/step - loss: 0.0384 - mae: 0.1486 - val_loss: 0.0816 - val_mae: 0.2284\n",
      "Epoch 13/50\n",
      "765/765 [==============================] - 2s 3ms/step - loss: 0.0305 - mae: 0.1363 - val_loss: 0.0999 - val_mae: 0.2621\n",
      "Epoch 14/50\n",
      "765/765 [==============================] - 2s 3ms/step - loss: 0.0579 - mae: 0.1844 - val_loss: 0.0777 - val_mae: 0.2134\n",
      "Epoch 15/50\n",
      "765/765 [==============================] - 2s 3ms/step - loss: 0.0409 - mae: 0.1560 - val_loss: 0.0689 - val_mae: 0.2076\n",
      "Epoch 16/50\n",
      "765/765 [==============================] - 2s 3ms/step - loss: 0.0285 - mae: 0.1304 - val_loss: 0.1248 - val_mae: 0.2828\n",
      "Epoch 17/50\n",
      "765/765 [==============================] - 3s 3ms/step - loss: 0.0354 - mae: 0.1465 - val_loss: 0.0922 - val_mae: 0.2398\n",
      "Epoch 18/50\n",
      "765/765 [==============================] - 3s 3ms/step - loss: 0.0246 - mae: 0.1220 - val_loss: 0.1287 - val_mae: 0.2941\n",
      "Epoch 19/50\n",
      "765/765 [==============================] - 2s 3ms/step - loss: 0.0285 - mae: 0.1296 - val_loss: 0.1012 - val_mae: 0.2566\n",
      "Epoch 20/50\n",
      "765/765 [==============================] - 3s 3ms/step - loss: 0.0478 - mae: 0.1678 - val_loss: 0.0839 - val_mae: 0.2349\n",
      "Epoch 21/50\n",
      "765/765 [==============================] - 2s 3ms/step - loss: 0.0230 - mae: 0.1173 - val_loss: 0.0611 - val_mae: 0.1950\n",
      "Epoch 22/50\n",
      "765/765 [==============================] - 3s 3ms/step - loss: 0.0416 - mae: 0.1588 - val_loss: 0.1025 - val_mae: 0.2552\n",
      "Epoch 23/50\n",
      "765/765 [==============================] - 3s 3ms/step - loss: 0.0340 - mae: 0.1405 - val_loss: 0.0937 - val_mae: 0.2430\n",
      "Epoch 24/50\n",
      "765/765 [==============================] - 3s 3ms/step - loss: 0.0326 - mae: 0.1430 - val_loss: 0.0911 - val_mae: 0.2483\n",
      "Epoch 25/50\n",
      "765/765 [==============================] - 2s 3ms/step - loss: 0.0290 - mae: 0.1319 - val_loss: 0.1243 - val_mae: 0.2822\n",
      "Epoch 26/50\n",
      "765/765 [==============================] - 2s 3ms/step - loss: 0.0602 - mae: 0.1902 - val_loss: 0.0781 - val_mae: 0.2179\n",
      "Epoch 27/50\n",
      "765/765 [==============================] - 2s 3ms/step - loss: 0.0253 - mae: 0.1249 - val_loss: 0.0739 - val_mae: 0.2244\n",
      "Epoch 28/50\n",
      "765/765 [==============================] - 2s 3ms/step - loss: 0.0268 - mae: 0.1290 - val_loss: 0.0965 - val_mae: 0.2512\n",
      "Epoch 29/50\n",
      "765/765 [==============================] - 2s 3ms/step - loss: 0.0401 - mae: 0.1535 - val_loss: 0.0888 - val_mae: 0.2360\n",
      "Epoch 30/50\n",
      "765/765 [==============================] - 2s 3ms/step - loss: 0.0418 - mae: 0.1556 - val_loss: 0.1300 - val_mae: 0.2827\n",
      "Epoch 31/50\n",
      "765/765 [==============================] - 2s 3ms/step - loss: 0.0267 - mae: 0.1270 - val_loss: 0.0843 - val_mae: 0.2219\n",
      "Epoch 32/50\n",
      "765/765 [==============================] - 2s 3ms/step - loss: 0.0285 - mae: 0.1247 - val_loss: 0.0797 - val_mae: 0.2236\n",
      "Epoch 33/50\n",
      "765/765 [==============================] - 2s 3ms/step - loss: 0.0334 - mae: 0.1412 - val_loss: 0.0960 - val_mae: 0.2457\n",
      "Epoch 34/50\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.0279 - mae: 0.1300 - val_loss: 0.0787 - val_mae: 0.2242\n",
      "Epoch 35/50\n",
      "765/765 [==============================] - 3s 3ms/step - loss: 0.0358 - mae: 0.1465 - val_loss: 0.2452 - val_mae: 0.4204\n",
      "Epoch 36/50\n",
      "765/765 [==============================] - 3s 3ms/step - loss: 0.0293 - mae: 0.1326 - val_loss: 0.0682 - val_mae: 0.2096\n",
      "Epoch 37/50\n",
      "765/765 [==============================] - 2s 3ms/step - loss: 0.0320 - mae: 0.1362 - val_loss: 0.1133 - val_mae: 0.2583\n",
      "Epoch 38/50\n",
      "765/765 [==============================] - 2s 3ms/step - loss: 0.0269 - mae: 0.1262 - val_loss: 0.1038 - val_mae: 0.2587\n",
      "Epoch 39/50\n",
      "765/765 [==============================] - 2s 3ms/step - loss: 0.0507 - mae: 0.1731 - val_loss: 0.0855 - val_mae: 0.2355\n",
      "Epoch 40/50\n",
      "765/765 [==============================] - 2s 3ms/step - loss: 0.0180 - mae: 0.1042 - val_loss: 0.0792 - val_mae: 0.2283\n",
      "Epoch 41/50\n",
      "765/765 [==============================] - 2s 3ms/step - loss: 0.0365 - mae: 0.1495 - val_loss: 0.0874 - val_mae: 0.2297\n",
      "Epoch 42/50\n",
      "765/765 [==============================] - 2s 3ms/step - loss: 0.0290 - mae: 0.1318 - val_loss: 0.0727 - val_mae: 0.2150\n",
      "Epoch 43/50\n",
      "765/765 [==============================] - 2s 3ms/step - loss: 0.0312 - mae: 0.1347 - val_loss: 0.0677 - val_mae: 0.2070\n",
      "Epoch 44/50\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.0201 - mae: 0.1134 - val_loss: 0.0695 - val_mae: 0.2074\n",
      "Epoch 45/50\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.0278 - mae: 0.1308 - val_loss: 0.0759 - val_mae: 0.2124\n",
      "Epoch 46/50\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.0295 - mae: 0.1339 - val_loss: 0.0703 - val_mae: 0.2102\n",
      "Epoch 47/50\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.0338 - mae: 0.1410 - val_loss: 0.1050 - val_mae: 0.2584\n",
      "Epoch 48/50\n",
      "765/765 [==============================] - 2s 3ms/step - loss: 0.0331 - mae: 0.1382 - val_loss: 0.0647 - val_mae: 0.2004\n",
      "Epoch 49/50\n",
      "765/765 [==============================] - 2s 3ms/step - loss: 0.0229 - mae: 0.1142 - val_loss: 0.0955 - val_mae: 0.2481\n",
      "Epoch 50/50\n",
      "765/765 [==============================] - 3s 3ms/step - loss: 0.0460 - mae: 0.1620 - val_loss: 0.0985 - val_mae: 0.2527\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d6e53bb940>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size = 1,\n",
    "    epochs = 50,\n",
    "    validation_split = 0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('uts.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('python')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45bf7f17dadebdc581538a04e72eec4af353140a24cf5b2bc98c1d568cfdbf7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
